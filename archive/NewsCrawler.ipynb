{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06d737d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance\n",
    "from yfinance.ticker import Ticker\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from pprint import pprint\n",
    "from transformers import pipeline, SummarizationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d9eeeb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsSummarizer:\n",
    "    \"\"\"\n",
    "    A utility class for retrieving, cleaning, and summarizing news articles \n",
    "    associated with a Yahoo Finance ticker symbol.\n",
    "\n",
    "    This class uses a Hugging Face summarization pipeline to generate concise\n",
    "    summaries for each article, and optionally produces a meta-summary \n",
    "    combining all individual summaries.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, summarizer: SummarizationPipeline, ticker: Ticker):\n",
    "        \"\"\"\n",
    "        Initialize the NewsSummarizer.\n",
    "\n",
    "        Args:\n",
    "            summarizer (SummarizationPipeline): A Hugging Face summarization pipeline.\n",
    "            ticker (Ticker): A Yahoo Finance Ticker object providing news data.\n",
    "        \"\"\"\n",
    "        # Use comma-free assignment (previous version had a trailing comma bug)\n",
    "        self.summarizer = summarizer\n",
    "        self.ticker = ticker\n",
    "        self.content_list = []\n",
    "        self.content_summaries = []\n",
    "        self.meta_summary = None\n",
    "\n",
    "    def _yfinance_aticle_crawler(self, news_obj: dict) -> dict[str, str]:\n",
    "        \"\"\"\n",
    "        Fetch the full HTML content of a Yahoo Finance news article.\n",
    "\n",
    "        Args:\n",
    "            news_obj (dict): A single news object from yfinance.ticker.news.\n",
    "\n",
    "        Returns:\n",
    "            dict[str, str]: A dictionary containing the article title, URL, and raw HTML text.\n",
    "        \"\"\"\n",
    "        # Determine the correct article URL (click-through or preview)\n",
    "        if news_obj[\"content\"][\"clickThroughUrl\"] is not None:\n",
    "            url = news_obj[\"content\"][\"clickThroughUrl\"][\"url\"]\n",
    "        else:\n",
    "            url = news_obj[\"content\"][\"previewUrl\"]\n",
    "\n",
    "        # Define realistic browser headers to avoid being blocked by the site\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:128.0) Gecko/20100101 Firefox/128.0\",\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "            \"Referer\": \"https://www.google.com/\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Upgrade-Insecure-Requests\": \"1\",\n",
    "            \"DNT\": \"1\",  # Do Not Track\n",
    "        }\n",
    "\n",
    "        # Request the article content\n",
    "        session = requests.Session()\n",
    "        response = session.get(url, headers=headers)\n",
    "        session.close()\n",
    "\n",
    "        return {\n",
    "            \"title\": news_obj[\"content\"][\"title\"],\n",
    "            \"url\": url,\n",
    "            \"response_text\": response.text\n",
    "        }\n",
    "\n",
    "    def _yfinance_article_cleaner(self, response_text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and extract relevant textual content from an article’s HTML.\n",
    "\n",
    "        Args:\n",
    "            response_text (str): Raw HTML text of the article.\n",
    "\n",
    "        Returns:\n",
    "            str: Cleaned plain text without irrelevant sections or HTML tags.\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(response_text, \"html.parser\")\n",
    "        plain_text = soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "        # Remove leading promotional content (e.g., “In this article:”)\n",
    "        pattern_leader = \"^.*In this article:\"\n",
    "        relevant_text = re.sub(pattern=pattern_leader, repl=\"\", string=plain_text)\n",
    "\n",
    "        # Cut off at known end markers that often indicate unrelated sections\n",
    "        for marker in [\n",
    "            \"Related articles\",\n",
    "            \"Read more\",\n",
    "            \"Continue Reading\",\n",
    "            \"Story Continues\",\n",
    "            \"View Comments\",\n",
    "        ]:\n",
    "            idx = relevant_text.rfind(marker)\n",
    "            if idx != -1:\n",
    "                relevant_text = relevant_text[:idx]\n",
    "\n",
    "        return relevant_text.strip()\n",
    "\n",
    "    def _yfinance_pull_clean(self, news_obj: dict) -> dict[str, str]:\n",
    "        \"\"\"\n",
    "        Retrieve and clean an article from Yahoo Finance.\n",
    "\n",
    "        Args:\n",
    "            news_obj (dict): A single Yahoo Finance news object.\n",
    "\n",
    "        Returns:\n",
    "            dict[str, str]: Dictionary with title, URL, raw HTML, and cleaned text.\n",
    "        \"\"\"\n",
    "        response_obj = self._yfinance_aticle_crawler(news_obj)\n",
    "        response_obj[\"clean_text\"] = self._yfinance_article_cleaner(response_obj[\"response_text\"])\n",
    "        return response_obj\n",
    "\n",
    "    def _content_summarizer(self) -> list[dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Summarize each cleaned article using the summarization model.\n",
    "\n",
    "        Returns:\n",
    "            list[dict[str, str]]: A list of summary dictionaries containing title, URL, and summary text.\n",
    "        \"\"\"\n",
    "        summaries: list[dict[str, str]] = []\n",
    "\n",
    "        for content in self.content_list:\n",
    "            clean_text = content[\"clean_text\"]\n",
    "            r = None\n",
    "            max_length = 200\n",
    "            min_length = 30\n",
    "\n",
    "            try:\n",
    "                word_count = len(clean_text.split(\" \"))\n",
    "\n",
    "                # Dynamically adjust summary lengths based on article size\n",
    "                if word_count < 200:\n",
    "                    max_length = int(word_count * 0.9)\n",
    "\n",
    "                if word_count < min_length:\n",
    "                    min_length = max_length\n",
    "\n",
    "                # Run summarization pipeline\n",
    "                r = self.summarizer(\n",
    "                    clean_text, min_length=min_length, max_length=max_length\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Summarization failed: {e}\")\n",
    "                pass\n",
    "            finally:\n",
    "                if r is not None:\n",
    "                    summary_obj = {\n",
    "                        \"title\": content[\"title\"],\n",
    "                        \"url\": content[\"url\"],\n",
    "                        \"summary_text\": r[0][\"summary_text\"],\n",
    "                    }\n",
    "                    summaries.append(summary_obj)\n",
    "\n",
    "        self.content_summaries = summaries\n",
    "\n",
    "    def _meta_summary(self):\n",
    "        \"\"\"\n",
    "        Generate a higher-level summary that summarizes all article summaries.\n",
    "        \"\"\"\n",
    "        if self.content_summaries:\n",
    "            # Combine all individual summaries into one string\n",
    "            summary_texts = [t[\"summary_text\"] for t in self.content_summaries]\n",
    "            summary_str = \" \".join(summary_texts)\n",
    "\n",
    "            min_length = 100\n",
    "            max_length = 1000\n",
    "            word_count = len(summary_str.split(\" \"))\n",
    "\n",
    "            # Dynamically adjust summary length parameters\n",
    "            if word_count < 1000:\n",
    "                max_length = int(word_count * 0.9)\n",
    "            if word_count < min_length:\n",
    "                min_length = max_length\n",
    "\n",
    "            # Generate the meta-summary\n",
    "            self.meta_summary = self.summarizer(\n",
    "                summary_str, min_length=min_length, max_length=max_length\n",
    "            )\n",
    "        else:\n",
    "            # If no summaries exist, generate them first\n",
    "            self.get_summaries(include_meta=True)\n",
    "\n",
    "    def get_summaries(self, include_meta: bool = False) -> dict:\n",
    "        \"\"\"\n",
    "        Retrieve all news for the given ticker, clean and summarize each one.\n",
    "\n",
    "        Args:\n",
    "            include_meta (bool, optional): If True, also generate a meta-summary. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing summaries and optionally the meta-summary.\n",
    "        \"\"\"\n",
    "        news_list = self.ticker.news\n",
    "\n",
    "        # Retrieve and clean each article\n",
    "        self.content_list = [self._yfinance_pull_clean(n) for n in news_list]\n",
    "\n",
    "        # Summarize the cleaned content\n",
    "        self._content_summarizer()\n",
    "\n",
    "        if include_meta:\n",
    "            self._meta_summary()\n",
    "\n",
    "    def get_meta_summary(self):\n",
    "        \"\"\"\n",
    "        Retrieve or generate a meta-summary summarizing all article summaries.\n",
    "        \"\"\"\n",
    "        if self.content_summaries:\n",
    "            if self.meta_summary is None:\n",
    "                self._meta_summary()\n",
    "        else:\n",
    "            self.get_summaries(include_meta=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db439e75",
   "metadata": {},
   "source": [
    "# Test Case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a0c4166b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "ticker = yfinance.Ticker('GOOG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "74c12e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_summarizer = NewsSummarizer(summarizer, ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6256c199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarization failed: index out of range in self\n"
     ]
    }
   ],
   "source": [
    "news_summarizer.get_summaries(include_meta=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "75643eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': \"Google's Gemini Enterprise offering is meant to go \"\n",
      "                  \"toe-to-toe with Microsoft's 365 Copilot. It will enable \"\n",
      "                  \"customers to use Google's Gemini to analyze corporate data \"\n",
      "                  'and access AI agents in one place. Google said users will '\n",
      "                  'be able to connect Gemini Enterprise to existing data '\n",
      "                  'sources.',\n",
      "  'title': 'Google launches Gemini Enterprise, taking aim at Microsoft, OpenAI',\n",
      "  'url': 'https://finance.yahoo.com/news/google-launches-gemini-enterprise-taking-aim-at-microsoft-openai-120020256.html'},\n",
      " {'summary_text': 'BofA Securities reiterated a Buy rating on the stock with a '\n",
      "                  'price target of $252. The rating affirmation follows '\n",
      "                  'emerging reports that Google is testing a redesign of its '\n",
      "                  'Gemini AI application. The move could help boost user '\n",
      "                  'interaction and expand use cases for Gemini.',\n",
      "  'title': 'BofA Reiterates Buy on Alphabet (GOOGL) as Google Tests Gemini AI '\n",
      "           'App Redesign',\n",
      "  'url': 'https://finance.yahoo.com/news/bofa-reiterates-buy-alphabet-googl-211243268.html'},\n",
      " {'summary_text': 'Broadcom Inc. (NASDAQ: AVGO ) is one of the Trending AI '\n",
      "                  'Stocks on Wall Street’s Radar. On October 6, KeyBanc '\n",
      "                  'reiterated an Overweight rating and $420.00 price target on '\n",
      "                  'the stock.',\n",
      "  'title': 'KeyBanc Reaffirms Buy on Broadcom (AVGO) Ahead of Earnings, Citing '\n",
      "           'Strong AI Demand',\n",
      "  'url': 'https://finance.yahoo.com/news/keybanc-reaffirms-buy-broadcom-avgo-211211287.html'},\n",
      " {'summary_text': 'Digital infrastructure provider Applied Digital '\n",
      "                  '(NASDAQ:APLD) beat Wall Street’s revenue expectations in Q3 '\n",
      "                  'CY2025, with sales up 5.8% year on year to $64.22 million. '\n",
      "                  'Its non-GAAP loss of $0.03 per share was 80.6% above '\n",
      "                  'analysts’ consensus estimates. Is now the time to buy '\n",
      "                  'Applied Digital? Find out in our full research report.',\n",
      "  'title': 'Applied Digital (NASDAQ:APLD) Reports Bullish Q3',\n",
      "  'url': 'https://finance.yahoo.com/news/applied-digital-nasdaq-apld-reports-201558442.html'},\n",
      " {'summary_text': 'Citi uses the “catalyst watch” tag when an analyst expects '\n",
      "                  'shares to perform well soon. Citi has said that it projects '\n",
      "                  '56% average annual spending growth between 2025 and 2029.',\n",
      "  'title': 'Buy This AI Stock Now, Says Citi. The Next 90 Days Are Key.',\n",
      "  'url': 'https://finance.yahoo.com/m/190ba86d-4b13-31e8-9fc4-1b89d6f95ce2/buy-this-ai-stock-now-says.html'},\n",
      " {'summary_text': 'Apple stock continues to target a fresh breakout to an '\n",
      "                  'all-time high. GOOG AAPL M',\n",
      "  'title': 'Apple Harvests Demand — And Picks This Fight With Meta, Google',\n",
      "  'url': 'https://finance.yahoo.com/m/300338ca-7dcf-377c-b740-ae2c72138b2d/apple-harvests-demand-%E2%80%94-and.html'},\n",
      " {'summary_text': 'UiPath stock surged more than 40% this week following a new '\n",
      "                  'collaboration with ChatGPT maker OpenAI on agentic AI '\n",
      "                  'technology. The rally actually started last week when '\n",
      "                  'UiPath announced integrations with Snowflake, Google '\n",
      "                  \"Gemini, Microsoft Azure, and Nvidia's AI platforms.\",\n",
      "  'title': 'Why UiPath Stock Surged (Again) This Week',\n",
      "  'url': 'https://finance.yahoo.com/news/why-uipath-stock-surged-again-193000361.html'},\n",
      " {'summary_text': 'Daniel Newman: Not all companies are created equal in the '\n",
      "                  'AI space. Tesla is probably the one that I continue to feel '\n",
      "                  'a little uncertain about, but I think long term will be '\n",
      "                  'very interesting.',\n",
      "  'title': 'These AI stocks could be the next winners',\n",
      "  'url': 'https://finance.yahoo.com/news/ai-stocks-could-next-winners-192240045.html'},\n",
      " {'summary_text': 'Many of Microsoft’s US data center regions are experiencing '\n",
      "                  'shortages of physical space or servers. New subscriptions '\n",
      "                  'for Azure cloud services are restricted in some crucial '\n",
      "                  'server-farm hubs. Microsoft has said it was unable to meet '\n",
      "                  'all of its customers’ cloud demand.',\n",
      "  'title': 'Microsoft Forecasts Show Data Center Crunch Persisting Into 2026',\n",
      "  'url': 'https://finance.yahoo.com/news/microsoft-forecasts-show-data-center-191543276.html'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(news_summarizer.content_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e97760ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'Broadcom Inc. (NASDAQ: AVGO ) is one of the Trending AI '\n",
      "                  'Stocks on Wall Street’s Radar. BofA Securities reiterated a '\n",
      "                  'Buy rating on the stock with a price target of $252. '\n",
      "                  \"Applied Digital beat Wall Street's revenue expectations in \"\n",
      "                  'Q3 CY2025, with sales up 5.8% year on year to $64.22 '\n",
      "                  'million. UiPath stock surged more than 40% this week '\n",
      "                  'following a new collaboration with ChatGPT maker OpenAI.'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(news_summarizer.meta_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_final_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
